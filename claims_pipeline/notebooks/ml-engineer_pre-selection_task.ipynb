{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation function - this is not part of the task, and is only to simulate pulling from a SQL database\n",
    "# Don't worry about changing this - it's just to make the notebook a little easier!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def collect_from_database(query: str) -> pd.DataFrame:\n",
    "    print(f\"Executing: {query}\")\n",
    "    n_rows = 10_000\n",
    "    n_features = 16\n",
    "    features, labels = make_classification(\n",
    "        n_samples=n_rows,\n",
    "        n_features=16,\n",
    "        n_informative=7,\n",
    "        n_redundant=4,\n",
    "        n_repeated=3,\n",
    "        n_classes=2,\n",
    "        class_sep=1.2,\n",
    "        flip_y=0.035,  # Randomly invert y for added noise\n",
    "        weights=[0.85, 0.15],\n",
    "        random_state=1889,\n",
    "    )\n",
    "    df = pd.DataFrame(features, columns=[f\"numeric_{i+1}\" for i in range(n_features)])\n",
    "    df.insert(value=labels, loc=0, column=\"claim_status\")\n",
    "    df = df.rename(\n",
    "        columns={\n",
    "            \"numeric_1\": \"age\",\n",
    "            \"numeric_2\": \"height_cm\",\n",
    "            \"numeric_3\": \"weight_kg\",\n",
    "            \"numeric_4\": \"income\",\n",
    "            \"numeric_5\": \"financial_hist_1\",\n",
    "            \"numeric_6\": \"financial_hist_2\",\n",
    "            \"numeric_7\": \"financial_hist_3\",\n",
    "            \"numeric_8\": \"financial_hist_4\",\n",
    "            \"numeric_9\": \"credit_score_1\",\n",
    "            \"numeric_10\": \"credit_score_2\",\n",
    "            \"numeric_11\": \"credit_score_3\",\n",
    "            \"numeric_12\": \"insurance_hist_1\",\n",
    "            \"numeric_13\": \"insurance_hist_2\",\n",
    "            \"numeric_14\": \"insurance_hist_3\",\n",
    "            \"numeric_15\": \"insurance_hist_4\",\n",
    "            \"numeric_16\": \"insurance_hist_5\",\n",
    "        }\n",
    "    )\n",
    "    df[\"age\"] = MinMaxScaler(feature_range=(18, 95)).fit_transform(\n",
    "        df[\"age\"].values[:, None]\n",
    "    )\n",
    "    df[\"age\"] = df[\"age\"].astype(\"int\")\n",
    "    df[\"height_cm\"] = MinMaxScaler(feature_range=(140, 210)).fit_transform(\n",
    "        df[\"height_cm\"].values[:, None]\n",
    "    )\n",
    "    df[\"height_cm\"] = df[\"height_cm\"].astype(\"int\")\n",
    "    df[\"weight_kg\"] = MinMaxScaler(feature_range=(45, 125)).fit_transform(\n",
    "        df[\"weight_kg\"].values[:, None]\n",
    "    )\n",
    "    df[\"weight_kg\"] = df[\"weight_kg\"].astype(\"int\")\n",
    "    df[\"income\"] = MinMaxScaler(feature_range=(0, 250_000)).fit_transform(\n",
    "        df[\"income\"].values[:, None]\n",
    "    )\n",
    "    df[\"income\"] = df[\"income\"].astype(\"int\")\n",
    "    df[\"credit_score_1\"] = MinMaxScaler(feature_range=(0, 999)).fit_transform(\n",
    "        df[\"credit_score_1\"].values[:, None]\n",
    "    )\n",
    "    df[\"credit_score_1\"] = df[\"credit_score_1\"].astype(\"int\")\n",
    "    df[\"credit_score_2\"] = MinMaxScaler(feature_range=(0, 700)).fit_transform(\n",
    "        df[\"credit_score_2\"].values[:, None]\n",
    "    )\n",
    "    df[\"credit_score_2\"] = df[\"credit_score_2\"].astype(\"int\")\n",
    "    df[\"credit_score_3\"] = MinMaxScaler(feature_range=(0, 710)).fit_transform(\n",
    "        df[\"credit_score_3\"].values[:, None]\n",
    "    )\n",
    "    df[\"credit_score_3\"] = df[\"credit_score_3\"].astype(\"int\")\n",
    "    df[\"bmi\"] = (df[\"weight_kg\"] / ((df[\"height_cm\"] / 100) ** 2)).astype(\"int\")\n",
    "    df[\"gender\"] = np.where(\n",
    "        df[\"claim_status\"] == 0,\n",
    "        np.random.choice([1, 0], size=(n_rows), p=[0.46, 0.54]),\n",
    "        np.random.choice([1, 0], size=(n_rows), p=[0.52, 0.48]),\n",
    "    )\n",
    "    df[\"marital_status\"] = np.random.choice(\n",
    "        [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n",
    "        size=(n_rows),\n",
    "        p=[0.2, 0.15, 0.1, 0.25, 0.15, 0.15],\n",
    "    )\n",
    "    df[\"occupation\"] = np.random.choice(\n",
    "        [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"], size=(n_rows)\n",
    "    )\n",
    "    df[\"location\"] = np.random.choice(list(string.ascii_uppercase), size=(n_rows))\n",
    "    df[\"prev_claim_rejected\"] = np.where(\n",
    "        df[\"claim_status\"] == 0,\n",
    "        np.random.choice([1, 0], size=(n_rows), p=[0.08, 0.92]),\n",
    "        np.random.choice([1, 0], size=(n_rows), p=[0.16, 0.84]),\n",
    "    )\n",
    "    df[\"known_health_conditions\"] = np.random.choice(\n",
    "        [1, 0], size=(n_rows), p=[0.06, 0.94]\n",
    "    )\n",
    "    df[\"uk_residence\"] = np.random.choice([1, 0], size=(n_rows), p=[0.76, 0.24])\n",
    "    df[\"family_history_1\"] = np.random.choice([1, 0], size=(n_rows), p=[0.22, 0.78])\n",
    "    df[\"family_history_2\"] = np.random.choice([1, 0], size=(n_rows), p=[0.25, 0.75])\n",
    "    df[\"family_history_3\"] = np.random.choice(\n",
    "        [1, None, 0], size=(n_rows), p=[0.12, 0.81, 0.07]\n",
    "    )\n",
    "    df[\"family_history_4\"] = np.random.choice([1, 0], size=(n_rows), p=[0.27, 0.73])\n",
    "    df[\"family_history_5\"] = np.random.choice([1, 0], size=(n_rows), p=[0.31, 0.69])\n",
    "    df[\"product_var_1\"] = np.random.choice([1, 0], size=(n_rows), p=[0.38, 0.62])\n",
    "    df[\"product_var_2\"] = np.random.choice([1, 0], size=(n_rows), p=[0.55, 0.45])\n",
    "    df[\"product_var_3\"] = np.random.choice(\n",
    "        [\"A\", \"B\", \"C\", \"D\"], size=(n_rows), p=[0.23, 0.28, 0.31, 0.18]\n",
    "    )\n",
    "    df[\"product_var_4\"] = np.random.choice([1, 0], size=(n_rows), p=[0.76, 0.24])\n",
    "    df[\"health_status\"] = np.random.randint(1, 5, size=(n_rows))\n",
    "    df[\"driving_record\"] = np.random.randint(1, 5, size=(n_rows))\n",
    "    df[\"previous_claim_rate\"] = np.where(\n",
    "        df[\"claim_status\"] == 0,\n",
    "        np.random.choice(\n",
    "            [1, 2, 3, 4, 5], size=(n_rows), p=[0.48, 0.29, 0.12, 0.08, 0.03]\n",
    "        ),\n",
    "        np.random.choice(\n",
    "            [1, 2, 3, 4, 5], size=(n_rows), p=[0.12, 0.28, 0.34, 0.19, 0.07]\n",
    "        ),\n",
    "    )\n",
    "    df[\"education_level\"] = np.random.randint(0, 7, size=(n_rows))\n",
    "    df[\"income level\"] = pd.cut(df[\"income\"], bins=5, labels=False, include_lowest=True)\n",
    "    df[\"n_dependents\"] = np.random.choice(\n",
    "        [1, 2, 3, 4, 5], size=(n_rows), p=[0.23, 0.32, 0.27, 0.11, 0.07]\n",
    "    )\n",
    "    df[\"employment_type\"] = np.random.choice(\n",
    "        [1, None, 0], size=(n_rows), p=[0.16, 0.7, 0.14]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-selection Task:** Alex's Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap # this module is not supported for python 3.11 and later. 3.10 goes eol October 2026\n",
    "import xgboost as xgb\n",
    "from scipy import stats\n",
    "\n",
    "# from sklearn.datasets import *\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "# Seed\n",
    "np.random.seed(1889)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moved env variables to config/dev/data_collection.yaml\n",
    "# Client secret moved to Githud Secrets. In future, this should be migrated to a more secure secrets management service such as SSM Parameter Store or Azure Key Vault."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing: SELECT * FROM CLAIMS.DS_DATASET\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim_status</th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>income</th>\n",
       "      <th>financial_hist_1</th>\n",
       "      <th>financial_hist_2</th>\n",
       "      <th>financial_hist_3</th>\n",
       "      <th>financial_hist_4</th>\n",
       "      <th>credit_score_1</th>\n",
       "      <th>...</th>\n",
       "      <th>product_var_2</th>\n",
       "      <th>product_var_3</th>\n",
       "      <th>product_var_4</th>\n",
       "      <th>health_status</th>\n",
       "      <th>driving_record</th>\n",
       "      <th>previous_claim_rate</th>\n",
       "      <th>education_level</th>\n",
       "      <th>income level</th>\n",
       "      <th>n_dependents</th>\n",
       "      <th>employment_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>182</td>\n",
       "      <td>80</td>\n",
       "      <td>155292</td>\n",
       "      <td>1.408797</td>\n",
       "      <td>-0.573517</td>\n",
       "      <td>-0.464914</td>\n",
       "      <td>3.249624</td>\n",
       "      <td>519</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>185</td>\n",
       "      <td>86</td>\n",
       "      <td>169232</td>\n",
       "      <td>2.402368</td>\n",
       "      <td>1.135213</td>\n",
       "      <td>0.780797</td>\n",
       "      <td>3.075214</td>\n",
       "      <td>500</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>169</td>\n",
       "      <td>82</td>\n",
       "      <td>173856</td>\n",
       "      <td>-1.182139</td>\n",
       "      <td>0.337877</td>\n",
       "      <td>1.754185</td>\n",
       "      <td>1.932426</td>\n",
       "      <td>422</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>179</td>\n",
       "      <td>76</td>\n",
       "      <td>150264</td>\n",
       "      <td>1.107481</td>\n",
       "      <td>-1.123652</td>\n",
       "      <td>1.966727</td>\n",
       "      <td>-0.130857</td>\n",
       "      <td>297</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>167</td>\n",
       "      <td>86</td>\n",
       "      <td>118449</td>\n",
       "      <td>-0.985055</td>\n",
       "      <td>-0.604696</td>\n",
       "      <td>-0.650367</td>\n",
       "      <td>-1.329931</td>\n",
       "      <td>440</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>184</td>\n",
       "      <td>89</td>\n",
       "      <td>147966</td>\n",
       "      <td>-0.532343</td>\n",
       "      <td>1.668309</td>\n",
       "      <td>1.672834</td>\n",
       "      <td>1.083610</td>\n",
       "      <td>423</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>161</td>\n",
       "      <td>64</td>\n",
       "      <td>124218</td>\n",
       "      <td>-1.594298</td>\n",
       "      <td>-0.059563</td>\n",
       "      <td>-2.565507</td>\n",
       "      <td>-1.216361</td>\n",
       "      <td>392</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>170</td>\n",
       "      <td>96</td>\n",
       "      <td>109517</td>\n",
       "      <td>-0.867955</td>\n",
       "      <td>-1.461820</td>\n",
       "      <td>-0.543154</td>\n",
       "      <td>-1.298344</td>\n",
       "      <td>430</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>166</td>\n",
       "      <td>87</td>\n",
       "      <td>182091</td>\n",
       "      <td>-2.868900</td>\n",
       "      <td>-0.193578</td>\n",
       "      <td>2.230964</td>\n",
       "      <td>2.590524</td>\n",
       "      <td>349</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>166</td>\n",
       "      <td>78</td>\n",
       "      <td>171201</td>\n",
       "      <td>-2.935281</td>\n",
       "      <td>1.091528</td>\n",
       "      <td>1.392789</td>\n",
       "      <td>2.102653</td>\n",
       "      <td>417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>A</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   claim_status  age  height_cm  weight_kg  income  financial_hist_1  \\\n",
       "0             0   49        182         80  155292          1.408797   \n",
       "1             0   55        185         86  169232          2.402368   \n",
       "2             0   55        169         82  173856         -1.182139   \n",
       "3             0   43        179         76  150264          1.107481   \n",
       "4             0   56        167         86  118449         -0.985055   \n",
       "5             0   47        184         89  147966         -0.532343   \n",
       "6             0   58        161         64  124218         -1.594298   \n",
       "7             0   55        170         96  109517         -0.867955   \n",
       "8             0   55        166         87  182091         -2.868900   \n",
       "9             0   52        166         78  171201         -2.935281   \n",
       "\n",
       "   financial_hist_2  financial_hist_3  financial_hist_4  credit_score_1  ...  \\\n",
       "0         -0.573517         -0.464914          3.249624             519  ...   \n",
       "1          1.135213          0.780797          3.075214             500  ...   \n",
       "2          0.337877          1.754185          1.932426             422  ...   \n",
       "3         -1.123652          1.966727         -0.130857             297  ...   \n",
       "4         -0.604696         -0.650367         -1.329931             440  ...   \n",
       "5          1.668309          1.672834          1.083610             423  ...   \n",
       "6         -0.059563         -2.565507         -1.216361             392  ...   \n",
       "7         -1.461820         -0.543154         -1.298344             430  ...   \n",
       "8         -0.193578          2.230964          2.590524             349  ...   \n",
       "9          1.091528          1.392789          2.102653             417  ...   \n",
       "\n",
       "   product_var_2  product_var_3  product_var_4  health_status  driving_record  \\\n",
       "0              1              B              1              1               3   \n",
       "1              1              C              1              4               3   \n",
       "2              1              B              1              3               3   \n",
       "3              0              C              1              4               3   \n",
       "4              0              B              1              1               3   \n",
       "5              0              C              1              3               3   \n",
       "6              1              C              1              3               3   \n",
       "7              1              D              0              3               4   \n",
       "8              1              D              1              1               4   \n",
       "9              0              A              1              1               1   \n",
       "\n",
       "   previous_claim_rate  education_level  income level  n_dependents  \\\n",
       "0                    1                5             3             3   \n",
       "1                    3                6             3             4   \n",
       "2                    2                3             3             4   \n",
       "3                    1                4             3             4   \n",
       "4                    3                4             2             5   \n",
       "5                    1                0             2             1   \n",
       "6                    1                5             2             3   \n",
       "7                    4                4             2             2   \n",
       "8                    2                5             3             4   \n",
       "9                    2                2             3             1   \n",
       "\n",
       "  employment_type  \n",
       "0            None  \n",
       "1            None  \n",
       "2            None  \n",
       "3            None  \n",
       "4            None  \n",
       "5            None  \n",
       "6               0  \n",
       "7            None  \n",
       "8            None  \n",
       "9            None  \n",
       "\n",
       "[10 rows x 41 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_from_database = collect_from_database(\"SELECT * FROM CLAIMS.DS_DATASET\")\n",
    "\n",
    "dataset_from_database.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['claim_status', 'age', 'height_cm', 'weight_kg', 'income',\n",
       "       'financial_hist_1', 'financial_hist_2', 'financial_hist_3',\n",
       "       'financial_hist_4', 'credit_score_1', 'credit_score_2',\n",
       "       'credit_score_3', 'insurance_hist_1', 'insurance_hist_2',\n",
       "       'insurance_hist_3', 'insurance_hist_4', 'insurance_hist_5', 'bmi',\n",
       "       'gender', 'marital_status', 'occupation', 'location',\n",
       "       'prev_claim_rejected', 'known_health_conditions', 'uk_residence',\n",
       "       'family_history_1', 'family_history_2', 'family_history_3',\n",
       "       'family_history_4', 'family_history_5', 'product_var_1',\n",
       "       'product_var_2', 'product_var_3', 'product_var_4', 'health_status',\n",
       "       'driving_record', 'previous_claim_rate', 'education_level',\n",
       "       'income level', 'n_dependents', 'employment_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_from_database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim_status\n",
       "0    8397\n",
       "1    1603\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check distribution of labels\n",
    "dataset_from_database[\"claim_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally save CSV locally\n",
    "\n",
    "dataset_from_database.to_csv(\"../data/dataset_from_database.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = dataset_from_database.isnull().sum()\n",
    "percent = (\n",
    "    dataset_from_database.isnull().sum() / dataset_from_database.isnull().count() * 100\n",
    ")\n",
    "missing_df = pd.concat([total, percent], axis=1, keys=[\"Total\", \"Percent\"])\n",
    "types = []\n",
    "for col in dataset_from_database.columns:\n",
    "    dtype = str(dataset_from_database[col].dtype)\n",
    "    types.append(dtype)\n",
    "missing_df[\"Types\"] = types\n",
    "dataset_from_database_no_missing_values = pd.DataFrame()\n",
    "dataset_from_database_no_missing_values = dataset_from_database.drop(\n",
    "    columns=[\"family_history_3\", \"employment_type\"]\n",
    ")\n",
    "dataset_from_database.drop(\n",
    "    columns=[\"family_history_3\", \"employment_type\"], inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: claim_status, dtype: int64\n",
      "Column: age, dtype: int64\n",
      "Column: height_cm, dtype: int64\n",
      "Column: weight_kg, dtype: int64\n",
      "Column: income, dtype: int64\n",
      "Column: financial_hist_1, dtype: float64\n",
      "Column: financial_hist_2, dtype: float64\n",
      "Column: financial_hist_3, dtype: float64\n",
      "Column: financial_hist_4, dtype: float64\n",
      "Column: credit_score_1, dtype: int64\n",
      "Column: credit_score_2, dtype: int64\n",
      "Column: credit_score_3, dtype: int64\n",
      "Column: insurance_hist_1, dtype: float64\n",
      "Column: insurance_hist_2, dtype: float64\n",
      "Column: insurance_hist_3, dtype: float64\n",
      "Column: insurance_hist_4, dtype: float64\n",
      "Column: insurance_hist_5, dtype: float64\n",
      "Column: bmi, dtype: int64\n",
      "Column: gender, dtype: int64\n",
      "Column: marital_status, dtype: object\n",
      "Column: occupation, dtype: object\n",
      "Column: location, dtype: object\n",
      "Column: prev_claim_rejected, dtype: int64\n",
      "Column: known_health_conditions, dtype: int64\n",
      "Column: uk_residence, dtype: int64\n",
      "Column: family_history_1, dtype: int64\n",
      "Column: family_history_2, dtype: int64\n",
      "Column: family_history_4, dtype: int64\n",
      "Column: family_history_5, dtype: int64\n",
      "Column: product_var_1, dtype: int64\n",
      "Column: product_var_2, dtype: int64\n",
      "Column: product_var_3, dtype: object\n",
      "Column: product_var_4, dtype: int64\n",
      "Column: health_status, dtype: int32\n",
      "Column: driving_record, dtype: int32\n",
      "Column: previous_claim_rate, dtype: int64\n",
      "Column: education_level, dtype: int32\n",
      "Column: income level, dtype: int64\n",
      "Column: n_dependents, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in dataset_from_database_no_missing_values.columns:\n",
    "    print(f\"Column: {i}, dtype: {dataset_from_database[i].dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "claim_status                  int64\n",
       "age                           int64\n",
       "height_cm                     int64\n",
       "weight_kg                     int64\n",
       "income                        int64\n",
       "financial_hist_1            float64\n",
       "financial_hist_2            float64\n",
       "financial_hist_3            float64\n",
       "financial_hist_4            float64\n",
       "credit_score_1                int64\n",
       "credit_score_2                int64\n",
       "credit_score_3                int64\n",
       "insurance_hist_1            float64\n",
       "insurance_hist_2            float64\n",
       "insurance_hist_3            float64\n",
       "insurance_hist_4            float64\n",
       "insurance_hist_5            float64\n",
       "bmi                           int64\n",
       "gender                     category\n",
       "marital_status             category\n",
       "occupation                 category\n",
       "location                   category\n",
       "prev_claim_rejected        category\n",
       "known_health_conditions    category\n",
       "uk_residence               category\n",
       "family_history_1           category\n",
       "family_history_2           category\n",
       "family_history_4           category\n",
       "family_history_5           category\n",
       "product_var_1              category\n",
       "product_var_2              category\n",
       "product_var_3              category\n",
       "product_var_4                 int64\n",
       "health_status              category\n",
       "driving_record             category\n",
       "previous_claim_rate        category\n",
       "education_level            category\n",
       "income level               category\n",
       "n_dependents               category\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_numerical = [\n",
    "    \"gender\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"location\",\n",
    "    \"prev_claim_rejected\",\n",
    "    \"known_health_conditions\",\n",
    "    \"uk_residence\",\n",
    "    \"family_history_1\",\n",
    "    \"family_history_2\",\n",
    "    \"family_history_4\",\n",
    "    \"family_history_5\",\n",
    "    \"product_var_1\",\n",
    "    \"product_var_2\",\n",
    "    \"product_var_3\",\n",
    "    \"health_status\",\n",
    "    \"driving_record\",\n",
    "    \"previous_claim_rate\",\n",
    "    \"education_level\",\n",
    "    \"income level\",\n",
    "    \"n_dependents\",\n",
    "]\n",
    "\n",
    "for column in non_numerical:\n",
    "    dataset_from_database[column] = dataset_from_database[column].astype(\"category\")\n",
    "\n",
    "dataset_from_database.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x28ed6f82260>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.pairplot(dataset_from_database, hue=\"claim_status\") # Takes 2-3 minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = dataset_from_database[\n",
    "    dataset_from_database.columns.drop(\n",
    "        [\"product_var_3\", \"marital_status\", \"occupation\", \"location\"]\n",
    "    )\n",
    "].corr()\n",
    "diag_mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Create axes and colourmap\n",
    "fig, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    mask=diag_mask,\n",
    "    cmap=cmap,\n",
    "    vmax=0.3,\n",
    "    center=0,\n",
    "    square=True,\n",
    "    linewidths=0.5,\n",
    "    cbar_kws={\"shrink\": 0.5},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in [\n",
    "    \"age\",\n",
    "    \"height_cm\",\n",
    "    \"weight_kg\",\n",
    "    \"income\",\n",
    "    \"financial_hist_1\",\n",
    "    \"financial_hist_2\",\n",
    "    \"financial_hist_3\",\n",
    "    \"financial_hist_4\",\n",
    "    \"credit_score_1\",\n",
    "    \"credit_score_2\",\n",
    "    \"credit_score_3\",\n",
    "    \"insurance_hist_1\",\n",
    "    \"insurance_hist_2\",\n",
    "    \"insurance_hist_3\",\n",
    "    \"insurance_hist_4\",\n",
    "]:\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(6, 4))\n",
    "    sns.boxplot(data=dataset_from_database, y=column, orient=\"v\", ax=ax[0])\n",
    "    sns.histplot(dataset_from_database, x=column, kde=True, ax=ax[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_plots = dataset_from_database.columns.drop(\n",
    "    [\n",
    "        \"claim_status\",\n",
    "        \"financial_hist_1\",\n",
    "        \"financial_hist_2\",\n",
    "        \"financial_hist_3\",\n",
    "        \"financial_hist_4\",\n",
    "        \"insurance_hist_1\",\n",
    "        \"insurance_hist_2\",\n",
    "        \"insurance_hist_3\",\n",
    "        \"insurance_hist_4\",\n",
    "        \"insurance_hist_5\",\n",
    "        \"income\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the Dataframe into labels and features\n",
    "X, y = (\n",
    "    dataset_from_database.drop(\"claim_status\", axis=1),\n",
    "    dataset_from_database[[\"claim_status\"]],\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1889\n",
    ")\n",
    "\n",
    "# Build the evaluation set & metric list\n",
    "eval_set = [(X_train, y_train)]\n",
    "eval_metrics = [\"auc\", \"rmse\", \"logloss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\", eval_metric=eval_metrics, enable_categorical=True\n",
    ")\n",
    "\n",
    "model.fit(X_test, y_test, eval_set=eval_set, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_preds = model.predict(X_train)\n",
    "test_class_preds = model.predict(X_test)\n",
    "train_prob_preds = model.predict_proba(X_train)[:, 1]\n",
    "test_prob_preds = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y = np.array(y_train)\n",
    "y = y.astype(int)\n",
    "yhat = np.array(train_class_preds)\n",
    "yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)\n",
    "training_data_kappa_score = round(cohen_kappa_score(yhat, y, weights=\"quadratic\"), 2)\n",
    "print(f\"The Cohen Kappa score on the training data is: {training_data_kappa_score}\")\n",
    "\n",
    "y = np.array(y_test)\n",
    "y = y.astype(int)\n",
    "yhat = np.array(test_class_preds)\n",
    "yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)\n",
    "test_data_kappa_score = round(cohen_kappa_score(yhat, y, weights=\"quadratic\"), 2)\n",
    "print(f\"The Cohen Kappa score on the test data is: {test_data_kappa_score}\")\n",
    "\n",
    "print()\n",
    "print(\"The accuracy on train dataset is: \", accuracy_score(y_train, train_class_preds))\n",
    "print(\"The accuracy on test dataset is: \", accuracy_score(y_test, test_class_preds))\n",
    "\n",
    "print()\n",
    "print(\"Train confusion matrix: \", confusion_matrix(y_train, train_class_preds))\n",
    "\n",
    "print()\n",
    "print(\"Test confusion matrix: \", confusion_matrix(y_test, test_class_preds))\n",
    "\n",
    "print()\n",
    "print(\"ROC on train data: \", roc_auc_score(y_train, train_prob_preds))\n",
    "print(\"ROC on test data: \", roc_auc_score(y_test, test_prob_preds))\n",
    "\n",
    "print()\n",
    "fpr, tpr, _ = roc_curve(y_test, test_prob_preds)\n",
    "random_fpr, random_tpr, _ = roc_curve(y_test, [0 for _ in range(len(y_test))])\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"XGBoost\")\n",
    "plt.plot(random_fpr, random_tpr, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Curve\")\n",
    "print(\"Train log loss: \", log_loss(y_train, train_prob_preds))\n",
    "print(\"Test log loss: \", log_loss(y_test, test_prob_preds))\n",
    "\n",
    "print()\n",
    "print(\"F1 score is: \", f1_score(y_test, test_class_preds))\n",
    "print(\"Precision is: \", precision_score(y_test, test_class_preds))\n",
    "print(\"Recall is: \", recall_score(y_test, test_class_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC Curve suggests amazing performance - espeically for an initial model! easier than expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(12, 6))\n",
    "xgb.plot_importance(model, ax=ax1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, ax2 = plt.subplots(figsize=(16, 16))\n",
    "xgb.plot_tree(model, rankdir=\"LR\", ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Model Training\n",
    "- Cross Vaildation tests to see if I can improve the model performance - not sure if it's useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_gridSearch = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=eval_metrics,\n",
    "        early_stopping_rounds=15,\n",
    "        enable_categorical=True,\n",
    "    ),\n",
    "    param_distributions={\n",
    "        \"n_estimators\": stats.randint(50, 500),\n",
    "        \"learning_rate\": stats.uniform(0.01, 0.75),\n",
    "        \"subsample\": stats.uniform(0.25, 0.75),\n",
    "        \"max_depth\": stats.randint(1, 8),\n",
    "        \"colsample_bytree\": stats.uniform(0.1, 0.75),\n",
    "        \"min_child_weight\": [1, 3, 5, 7, 9],\n",
    "    },\n",
    "    cv=5,\n",
    "    n_iter=100,\n",
    "    verbose=False,\n",
    "    scoring=\"roc_auc\",\n",
    ")\n",
    "\n",
    "parameter_gridSearch.fit(X_train, y_train, eval_set=eval_set, verbose=False)\n",
    "\n",
    "print(\"Best parameters are: \", parameter_gridSearch.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=eval_metrics,\n",
    "    early_stopping_rounds=15,\n",
    "    enable_categorical=True,\n",
    "    **parameter_gridSearch.best_params_,  # Not sure what this does, from StackOverflow\n",
    ")\n",
    "\n",
    "model3.fit(X_train, y_train, eval_set=eval_set, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    log_loss,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "train_class_preds2 = model3.predict(X_train)\n",
    "test_class_preds2 = model3.predict(X_test)\n",
    "train_prob_preds2 = model3.predict_proba(X_train)[:, 1]\n",
    "test_prob_preds2 = model3.predict_proba(X_test)[:, 1]\n",
    "\n",
    "y = np.array(y_train)\n",
    "y = y.astype(int)\n",
    "yhat = np.array(train_class_preds2)\n",
    "yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)\n",
    "kappa2 = round(cohen_kappa_score(yhat, y, weights=\"quadratic\"), 2)\n",
    "print(f\"The Cohen Kappa score on the training data is: {kappa2}\")\n",
    "\n",
    "y = np.array(y_test)\n",
    "y = y.astype(int)\n",
    "yhat = np.array(test_class_preds)\n",
    "yhat = np.clip(np.round(yhat), np.min(y), np.max(y)).astype(int)\n",
    "kappa2 = round(cohen_kappa_score(yhat, y, weights=\"quadratic\"), 2)\n",
    "print(f\"The Cohen Kappa score on the test data is: {kappa2}\")\n",
    "\n",
    "print()\n",
    "print(\"The accuracy on train dataset is: \", accuracy_score(y_train, train_class_preds2))\n",
    "print(\"The accuracy on test dataset is: \", accuracy_score(y_test, test_class_preds2))\n",
    "\n",
    "print()\n",
    "print(\"Train confusion matrix: \", confusion_matrix(y_train, train_class_preds2))\n",
    "\n",
    "print()\n",
    "print(\"Test confusion matrix: \", confusion_matrix(y_test, test_class_preds2))\n",
    "\n",
    "print()\n",
    "print(\"ROC on train data: \", roc_auc_score(y_train, train_prob_preds2))\n",
    "print(\"ROC on test data: \", roc_auc_score(y_test, test_prob_preds2))\n",
    "\n",
    "print()\n",
    "fpr, tpr, _ = roc_curve(y_test, test_prob_preds2)\n",
    "random_fpr, random_tpr, _ = roc_curve(y_test, [0 for _ in range(len(y_test))])\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, marker=\".\", label=\"XGBoost\")\n",
    "plt.plot(random_fpr, random_tpr, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Curve\")\n",
    "print(\"Train log loss: \", log_loss(y_train, train_prob_preds2))\n",
    "print(\"Test log loss: \", log_loss(y_test, test_prob_preds2))\n",
    "\n",
    "print()\n",
    "print(\"F1 score is: \", f1_score(y_test, test_class_preds2))\n",
    "print(\"Precision is: \", precision_score(y_test, test_class_preds2))\n",
    "print(\"Recall is: \", recall_score(y_test, test_class_preds2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "less performance in the ROC curve??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "shap_values = model3.get_booster().predict(\n",
    "    xgb.DMatrix(X_train, y_train, enable_categorical=True), pred_contribs=True\n",
    ")\n",
    "\n",
    "native_model = model3.get_booster()\n",
    "shap_values = native_model.predict(\n",
    "    xgb.DMatrix(X_train, y_train, enable_categorical=True), pred_contribs=True\n",
    ")\n",
    "\n",
    "shap.summary_plot(shap_values[:, :-1], X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save_model(\"xgboost_model_optimised_with_cross_validation.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLETask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
